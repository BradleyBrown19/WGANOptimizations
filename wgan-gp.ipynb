{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision.gan import *\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import pdb\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import torch.autograd as autograd\n",
    "path = Path()/'data'/'Images'\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import itertools as it\n",
    "#Ranger optimizer, stolen from: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger.py\n",
    "\n",
    "\n",
    "class Ranger(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-4, alpha=0.5, k=6, betas=(.9,0.999), eps=1e-8, weight_decay=0):\n",
    "        #parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "        \n",
    "        #prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params,defaults)\n",
    "        \n",
    "        #now we can get to work...\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "            #print(\"group step counter init\")\n",
    "                      \n",
    "        #look ahead params\n",
    "        self.alpha = alpha\n",
    "        self.k = k \n",
    "        \n",
    "        #radam buffer for state\n",
    "        self.radam_buffer = [[None,None,None] for ind in range(10)]\n",
    "        \n",
    "        #lookahead weights\n",
    "        self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
    "                                for group in self.param_groups]\n",
    "        \n",
    "        #don't use grad for lookahead weights\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        \n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "       \n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n",
    "        #Uncomment if you need to use the actual closure...\n",
    "        \n",
    "        #if closure is not None:\n",
    "            #loss = closure()\n",
    "            \n",
    "        #------------ radam\n",
    "        for group in self.param_groups:\n",
    "    \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "    \n",
    "                p_data_fp32 = p.data.float()\n",
    "    \n",
    "                state = self.state[p]\n",
    "    \n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "    \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "    \n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "    \n",
    "                state['step'] += 1\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "    \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "    \n",
    "                if N_sma > 5:                    \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "    \n",
    "                p.data.copy_(p_data_fp32)\n",
    "        \n",
    "        \n",
    "        #---------------- end radam step\n",
    "        \n",
    "        #look ahead tracking and updating if latest batch = k\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(self.alpha,p.data - q.data)\n",
    "                p.data.copy_(q.data)\n",
    "            \n",
    "        \n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.LSUN_BEDROOMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.vision.image as im\n",
    "def get_data(bs, size):\n",
    "    return (GANItemList.from_folder(path, noise_sz=128)\n",
    "               .split_none()\n",
    "               .label_from_func(noop)\n",
    "               .transform(tfms=[[crop_pad(size=size, row_pct=(0,1), col_pct=(0,1))], []], size=size, tfm_y=True)\n",
    "               .databunch(bs=bs)\n",
    "               .normalize(stats = [torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])], do_x=False, do_y=True))\n",
    "data = get_data(BATCH_SIZE, 64)\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "DIM=64\n",
    "OUTPUT_DIM=64*64*3\n",
    "\n",
    "class MyConvo2d(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, he_init = True,  stride = 1, bias = True):\n",
    "        super(MyConvo2d, self).__init__()\n",
    "        self.he_init = he_init\n",
    "        self.padding = int((kernel_size - 1)/2)\n",
    "        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=self.padding, bias = bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        return output\n",
    "\n",
    "class ConvMeanPool(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, he_init = True):\n",
    "        super(ConvMeanPool, self).__init__()\n",
    "        self.he_init = he_init\n",
    "        self.conv = MyConvo2d(input_dim, output_dim, kernel_size, he_init = self.he_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        output = (output[:,:,::2,::2] + output[:,:,1::2,::2] + output[:,:,::2,1::2] + output[:,:,1::2,1::2]) / 4\n",
    "        return output\n",
    "\n",
    "class MeanPoolConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, he_init = True):\n",
    "        super(MeanPoolConv, self).__init__()\n",
    "        self.he_init = he_init\n",
    "        self.conv = MyConvo2d(input_dim, output_dim, kernel_size, he_init = self.he_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        output = (output[:,:,::2,::2] + output[:,:,1::2,::2] + output[:,:,::2,1::2] + output[:,:,1::2,1::2]) / 4\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "class DepthToSpace(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super(DepthToSpace, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        self.block_size_sq = block_size*block_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.permute(0, 2, 3, 1)\n",
    "        (batch_size, input_height, input_width, input_depth) = output.size()\n",
    "        output_depth = int(input_depth / self.block_size_sq)\n",
    "        output_width = int(input_width * self.block_size)\n",
    "        output_height = int(input_height * self.block_size)\n",
    "        t_1 = output.reshape(batch_size, input_height, input_width, self.block_size_sq, output_depth)\n",
    "        spl = t_1.split(self.block_size, 3)\n",
    "        stacks = [t_t.reshape(batch_size,input_height,output_width,output_depth) for t_t in spl]\n",
    "        output = torch.stack(stacks,0).transpose(0,1).permute(0,2,1,3,4).reshape(batch_size,output_height,output_width,output_depth)\n",
    "        output = output.permute(0, 3, 1, 2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, he_init = True, bias=True):\n",
    "        super(UpSampleConv, self).__init__()\n",
    "        self.he_init = he_init\n",
    "        self.conv = MyConvo2d(input_dim, output_dim, kernel_size, he_init = self.he_init, bias=bias)\n",
    "        self.depth_to_space = DepthToSpace(2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        output = torch.cat((output, output, output, output), 1)\n",
    "        output = self.depth_to_space(output)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, resample=None, hw=DIM):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.resample = resample\n",
    "        self.bn1 = None\n",
    "        self.bn2 = None\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        if resample == 'down':\n",
    "            self.bn1 = nn.LayerNorm([input_dim, hw, hw])\n",
    "            self.bn2 = nn.LayerNorm([input_dim, hw, hw])\n",
    "        elif resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(input_dim)\n",
    "            self.bn2 = nn.BatchNorm2d(output_dim)\n",
    "        elif resample == None:\n",
    "            #TODO: ????\n",
    "            self.bn1 = nn.BatchNorm2d(output_dim)\n",
    "            self.bn2 = nn.LayerNorm([input_dim, hw, hw])\n",
    "        else:\n",
    "            raise Exception('invalid resample value')\n",
    "\n",
    "        if resample == 'down':\n",
    "            self.conv_shortcut = MeanPoolConv(input_dim, output_dim, kernel_size = 1, he_init = False)\n",
    "            self.conv_1 = MyConvo2d(input_dim, input_dim, kernel_size = kernel_size, bias = False)\n",
    "            self.conv_2 = ConvMeanPool(input_dim, output_dim, kernel_size = kernel_size)\n",
    "        elif resample == 'up':\n",
    "            self.conv_shortcut = UpSampleConv(input_dim, output_dim, kernel_size = 1, he_init = False)\n",
    "            self.conv_1 = UpSampleConv(input_dim, output_dim, kernel_size = kernel_size, bias = False)\n",
    "            self.conv_2 = MyConvo2d(output_dim, output_dim, kernel_size = kernel_size)\n",
    "        elif resample == None:\n",
    "            self.conv_shortcut = MyConvo2d(input_dim, output_dim, kernel_size = 1, he_init = False)\n",
    "            self.conv_1 = MyConvo2d(input_dim, input_dim, kernel_size = kernel_size, bias = False)\n",
    "            self.conv_2 = MyConvo2d(input_dim, output_dim, kernel_size = kernel_size)\n",
    "        else:\n",
    "            raise Exception('invalid resample value')\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.input_dim == self.output_dim and self.resample == None:\n",
    "            shortcut = input\n",
    "        else:\n",
    "            shortcut = self.conv_shortcut(input)\n",
    "\n",
    "        output = input\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv_1(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.conv_2(output)\n",
    "\n",
    "        return shortcut + output\n",
    "\n",
    "class ReLULayer(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(ReLULayer, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.linear(input)\n",
    "        output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "class FCGenerator(nn.Module):\n",
    "    def __init__(self, FC_DIM=512):\n",
    "        super(FCGenerator, self).__init__()\n",
    "        self.relulayer1 = ReLULayer(128, FC_DIM)\n",
    "        self.relulayer2 = ReLULayer(FC_DIM, FC_DIM)\n",
    "        self.relulayer3 = ReLULayer(FC_DIM, FC_DIM)\n",
    "        self.relulayer4 = ReLULayer(FC_DIM, FC_DIM)\n",
    "        self.linear = nn.Linear(FC_DIM, OUTPUT_DIM)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.relulayer1(input)\n",
    "        output = self.relulayer2(output)\n",
    "        output = self.relulayer3(output)\n",
    "        output = self.relulayer4(output)\n",
    "        output = self.linear(output)\n",
    "        output = self.tanh(output)\n",
    "        return output\n",
    "\n",
    "class GoodGenerator(nn.Module):\n",
    "    def __init__(self, dim=DIM,output_dim=OUTPUT_DIM):\n",
    "        super(GoodGenerator, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.ln1 = nn.Linear(128, 4*4*8*self.dim)\n",
    "        self.rb1 = ResidualBlock(8*self.dim, 8*self.dim, 3, resample = 'up')\n",
    "        self.rb2 = ResidualBlock(8*self.dim, 4*self.dim, 3, resample = 'up')\n",
    "        self.rb3 = ResidualBlock(4*self.dim, 2*self.dim, 3, resample = 'up')\n",
    "        self.rb4 = ResidualBlock(2*self.dim, 1*self.dim, 3, resample = 'up')\n",
    "        self.bn  = nn.BatchNorm2d(self.dim)\n",
    "\n",
    "        self.conv1 = MyConvo2d(1*self.dim, 3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.squeeze(-1).squeeze(-1)\n",
    "        #print(input.shape)\n",
    "        output = self.ln1(input)\n",
    "        output = output.view(-1, 8*self.dim, 4, 4)\n",
    "        output = self.rb1(output)\n",
    "        output = self.rb2(output)\n",
    "        output = self.rb3(output)\n",
    "        output = self.rb4(output)\n",
    "\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.conv1(output)\n",
    "        output = self.tanh(output)\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "class GoodDiscriminator(nn.Module):\n",
    "    def __init__(self, dim=DIM):\n",
    "        super(GoodDiscriminator, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.conv1 = MyConvo2d(3, self.dim, 3, he_init = False)\n",
    "        self.rb1 = ResidualBlock(self.dim, 2*self.dim, 3, resample = 'down', hw=DIM)\n",
    "        self.rb2 = ResidualBlock(2*self.dim, 4*self.dim, 3, resample = 'down', hw=int(DIM/2))\n",
    "        self.rb3 = ResidualBlock(4*self.dim, 8*self.dim, 3, resample = 'down', hw=int(DIM/4))\n",
    "        self.rb4 = ResidualBlock(8*self.dim, 8*self.dim, 3, resample = 'down', hw=int(DIM/8))\n",
    "        self.ln1 = nn.Linear(4*4*8*self.dim, 1)\n",
    "\n",
    "    def forward(self, input, dropout=0, intermediate_output=False):\n",
    "        output = input.contiguous()\n",
    "        output = output.view(-1, 3, DIM, DIM)\n",
    "        output = self.conv1(output)\n",
    "        output = self.rb1(output)\n",
    "        output = F.dropout(output, training=True, p=dropout)\n",
    "        output = self.rb2(output)\n",
    "        output = F.dropout(output, training=True, p=dropout)\n",
    "        output = self.rb3(output)\n",
    "        output = F.dropout(output, training=True, p=dropout)\n",
    "        output = self.rb4(output)\n",
    "        output = output.view(-1, 4*4*8*self.dim)\n",
    "        \n",
    "        if intermediate_output:\n",
    "            output2 = output\n",
    "            output = self.ln1(output)\n",
    "            output = output.view(-1)\n",
    "            return output, output2\n",
    "        else:\n",
    "            output = self.ln1(output)\n",
    "            output = output.view(-1)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLearner(Learner):\n",
    "    \"A `Learner` suitable for GANs.\"\n",
    "    def __init__(self, data:DataBunch, generator:nn.Module, critic:nn.Module, gen_loss_func:LossFunction,\n",
    "                 crit_loss_func:LossFunction, switcher:Callback=None, gen_first:bool=False, switch_eval:bool=True,\n",
    "                 show_img:bool=True, clip:float=None, **learn_kwargs):\n",
    "        gan = GANModule(generator, critic)\n",
    "        loss_func = GANLoss(gen_loss_func, crit_loss_func, gan)\n",
    "        switcher = ifnone(switcher, partial(FixedGANSwitcher, n_crit=5, n_gen=1))\n",
    "        super().__init__(data, gan, loss_func=loss_func, callback_fns=[switcher], **learn_kwargs)\n",
    "        trainer = GANTrainer(self, clip=clip, switch_eval=switch_eval, show_img=show_img)\n",
    "        self.gan_trainer = trainer\n",
    "        self.callbacks.append(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModule(nn.Module):\n",
    "    \"Wrapper around a `generator` and a `critic` to create a GAN.\"\n",
    "    def __init__(self, generator:nn.Module=None, critic:nn.Module=None, gen_mode:bool=False):\n",
    "        super().__init__()\n",
    "        self.gen_mode = gen_mode\n",
    "        if generator: self.generator,self.critic = generator,critic\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return self.generator(*args) if self.gen_mode else self.critic(*args)\n",
    "\n",
    "    def switch(self, gen_mode:bool=None):\n",
    "        \"Put the model in generator mode if `gen_mode`, in critic mode otherwise.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(GANModule):\n",
    "    \"Wrapper around `loss_funcC` (for the critic) and `loss_funcG` (for the generator).\"\n",
    "    def __init__(self, loss_funcG:Callable, loss_funcC:Callable, gan_model:GANModule):\n",
    "        super().__init__()\n",
    "        self.loss_funcG,self.loss_funcC,self.gan_model = loss_funcG,loss_funcC,gan_model\n",
    "\n",
    "    def generator(self, output, target):\n",
    "        \"Evaluate the `output` with the critic then uses `self.loss_funcG` to combine it with `target`.\"\n",
    "        fake_pred = self.gan_model.critic(output)\n",
    "        return self.loss_funcG(fake_pred, target, output)\n",
    "\n",
    "    def critic(self, real_pred, data):\n",
    "        \"Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.loss_funcD`.\"\n",
    "        fake_data = torch.split(data, BATCH_SIZE, dim=0)[0].requires_grad_(True)\n",
    "        real_data = torch.split(data, BATCH_SIZE, dim=0)[1].requires_grad_(True)\n",
    "        \n",
    "        #dw1, dwi1 = self.gan_model.critic(real_data, dropout=0.5, intermediate_output=True) # Perturb the input by applying dropout to hidden layers.\n",
    "        #dw2, dwi2 = self.gan_model.critic(real_data, dropout=0.5, intermediate_output=True)\n",
    "\n",
    "        fake_pred = self.gan_model.critic(fake_data)\n",
    "        return self.loss_funcC(real_pred, fake_pred, real_data, fake_data, self.gan_model.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANTrainer(LearnerCallback):\n",
    "    \"Handles GAN Training.\"\n",
    "    _order=-20\n",
    "    def __init__(self, learn:Learner, switch_eval:bool=False, clip:float=None, beta:float=0.98, gen_first:bool=False,\n",
    "                 show_img:bool=True):\n",
    "        super().__init__(learn)\n",
    "        self.switch_eval,self.clip,self.beta,self.gen_first,self.show_img = switch_eval,0.01,beta,gen_first,show_img\n",
    "        self.generator,self.critic = self.model.generator,self.model.critic\n",
    "\n",
    "    def _set_trainable(self):\n",
    "        train_model = self.generator if     self.gen_mode else self.critic\n",
    "        loss_model  = self.generator if not self.gen_mode else self.critic\n",
    "        requires_grad(train_model, True)\n",
    "        requires_grad(loss_model, False)\n",
    "        if self.switch_eval:\n",
    "            train_model.train()\n",
    "            loss_model.eval()\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"Create the optimizers for the generator and critic if necessary, initialize smootheners.\"\n",
    "        if not getattr(self,'opt_gen',None):\n",
    "            self.opt_gen = self.opt.new([nn.Sequential(*flatten_model(self.generator))])\n",
    "        else: self.opt_gen.lr,self.opt_gen.wd = self.opt.lr,self.opt.wd\n",
    "        if not getattr(self,'opt_critic',None):\n",
    "            self.opt_critic = self.opt.new([nn.Sequential(*flatten_model(self.critic))])\n",
    "        else: self.opt_critic.lr,self.opt_critic.wd = self.opt.lr,self.opt.wd\n",
    "        self.gen_mode = self.gen_first\n",
    "        self.switch(self.gen_mode)\n",
    "        self.closses,self.glosses = [],[]\n",
    "        self.smoothenerG,self.smoothenerC = SmoothenValue(self.beta),SmoothenValue(self.beta)\n",
    "        #self.recorder.no_val=True\n",
    "        self.recorder.add_metric_names(['gen_loss', 'disc_loss'])\n",
    "        self.imgs,self.titles = [],[]\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        \"Switch in generator mode for showing results.\"\n",
    "        self.switch(gen_mode=True)\n",
    "\n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        \"Clamp the weights with `self.clip` if it's not None, return the correct input.\"\n",
    "        if self.gen_mode:\n",
    "            return {'last_input':last_input,'last_target':last_target} \n",
    "        else:\n",
    "            #print(last_input.shape)\n",
    "            #print(last_target.shape)\n",
    "            fake = self.generator(last_input.requires_grad_(True))\n",
    "            #print(fake.shape)\n",
    "            #pdb.set_trace()\n",
    "            data = torch.cat((fake, last_target), dim=0)\n",
    "            return {'last_input':last_target,'last_target':data}\n",
    "\n",
    "    def on_backward_begin(self, last_loss, last_output, **kwargs):\n",
    "        \"Record `last_loss` in the proper list.\"\n",
    "        last_loss = last_loss.detach().cpu()\n",
    "        if self.gen_mode:\n",
    "            self.smoothenerG.add_value(last_loss)\n",
    "            self.glosses.append(self.smoothenerG.smooth)\n",
    "            self.last_gen = last_output.detach().cpu()\n",
    "        else:\n",
    "            self.smoothenerC.add_value(last_loss)\n",
    "            self.closses.append(self.smoothenerC.smooth)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, **kwargs):\n",
    "        \"Put the critic or the generator back to eval if necessary.\"\n",
    "        self.switch(self.gen_mode)\n",
    "\n",
    "    def on_epoch_end(self, pbar, epoch, last_metrics, **kwargs):\n",
    "        \"Put the various losses in the recorder and show a sample image.\"\n",
    "        if not hasattr(self, 'last_gen') or not self.show_img: return\n",
    "        data = self.learn.data\n",
    "        img = self.last_gen[0]\n",
    "        norm = getattr(data,'norm',False)\n",
    "        if norm and norm.keywords.get('do_y',False): img = data.denorm(img)\n",
    "        img = data.train_ds.y.reconstruct(img)\n",
    "        self.imgs.append(img)\n",
    "        self.titles.append(f'Epoch {epoch}')\n",
    "        pbar.show_imgs(self.imgs, self.titles)\n",
    "        return add_metrics(last_metrics, [getattr(self.smoothenerG,'smooth',None),getattr(self.smoothenerC,'smooth',None)])\n",
    "\n",
    "    def switch(self, gen_mode:bool=None):\n",
    "        \"Switch the model, if `gen_mode` is provided, in the desired mode.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode\n",
    "        self.opt.opt = self.opt_gen.opt if self.gen_mode else self.opt_critic.opt\n",
    "        self._set_trainable()\n",
    "        self.model.switch(gen_mode)\n",
    "        self.loss_func.switch(gen_mode)\n",
    "\n",
    "class FixedGANSwitcher(LearnerCallback):\n",
    "    \"Switcher to do `n_crit` iterations of the critic then `n_gen` iterations of the generator.\"\n",
    "    def __init__(self, learn:Learner, n_crit=3, n_gen=1):\n",
    "        super().__init__(learn)\n",
    "        self.n_crit,self.n_gen = 10,1\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"Initiate the iteration counts.\"\n",
    "        self.n_c,self.n_g = 0,0\n",
    "\n",
    "    def on_batch_end(self, iteration, **kwargs):\n",
    "        \"Switch the model if necessary.\"\n",
    "        if self.learn.gan_trainer.gen_mode:\n",
    "            self.n_g += 1\n",
    "            n_iter,n_in,n_out = self.n_gen,self.n_c,self.n_g\n",
    "        else:\n",
    "            self.n_c += 1\n",
    "            n_iter,n_in,n_out = self.n_crit,self.n_g,self.n_c\n",
    "        target = n_iter if isinstance(n_iter, int) else n_iter(n_in)\n",
    "        if target == n_out:\n",
    "            self.learn.gan_trainer.switch()\n",
    "            self.n_c,self.n_g = 0,0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "DIM = 64\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement()/BATCH_SIZE)).contiguous()\n",
    "    alpha = alpha.view(BATCH_SIZE, 3, DIM, DIM)\n",
    "    alpha = alpha.cuda()\n",
    "    \n",
    "    fake_data = fake_data.view(BATCH_SIZE, 3, DIM, DIM)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(BATCH_SIZE, -1)\n",
    "\n",
    "        # Derivatives of the gradient close to 0 can cause problems because of\n",
    "        # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "        # Return gradient penalty\n",
    "    return LAMBDA * ((gradients_norm - 1) ** 2).mean()\n",
    "\n",
    "LAMBDA2 = 2\n",
    "def consistency_term(dw1, dwi1, dw2, dwi2):  \n",
    "    d_wct_loss = (dw1 - dw2).norm(2, dim=0) + 0.1 * \\\n",
    "            (dwi1 - dwi2).norm(2, dim=1) - 0.1\n",
    "    return d_wct_loss.mean() * LAMBDA2\n",
    "\n",
    "class NoopLoss(nn.Module):\n",
    "    \"Just returns the mean of the `output`.\"\n",
    "    def forward(self, output, *args): \n",
    "        loss = -output.mean()\n",
    "        return loss\n",
    "\n",
    "class WassersteinLoss(nn.Module):\n",
    "    \"For WGAN.\"\n",
    "    def forward(self, real, fake, real_data, fake_data, disc): \n",
    "        gradient_penalty = calc_gradient_penalty(disc, real_data, fake_data)\n",
    "        #ct = consistency_term(dw1, dwi1, dw2, dwi2)\n",
    "        loss = (fake.mean() - real.mean() + gradient_penalty)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GoodGenerator()\n",
    "critic    = GoodDiscriminator()\n",
    "optar = partial(Ranger)\n",
    "\n",
    "wgan = GANLearner(data,\n",
    "                      generator=generator,\n",
    "                      critic=critic,\n",
    "                      gen_loss_func=NoopLoss(),\n",
    "                      crit_loss_func=WassersteinLoss(), switch_eval=False,opt_func = optar, wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wgan.fit(50, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.load('wgan-gp-bedroom-16e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.show_results(ds_type=DatasetType.Train, rows=16, figsize=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
